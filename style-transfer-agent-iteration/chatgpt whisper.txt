Gotcha. Here’s a clean way to wire **whisper.cpp on MacBook Air M3** into a **Python “always-listening” loop** that (1) detects when the user is speaking, (2) decides when they’ve finished the command (end-of-speech), and (3) hands you the recognized command text like “rotate the image to the right”.

### The approach (works great on Air M3)

* Use the **pywhispercpp** bindings for whisper.cpp so you can call it straight from Python. It ships an **“assistant”** example that already glues in VAD and continuous listening; you can adapt it or keep things minimal as below. ([PyPI][1])
* Do **VAD (voice activity detection)** in Python to know when to start/stop buffering audio. Two simple options:

  * `webrtcvad` (purely for speech/silence, very light). This is what the pywhispercpp assistant example uses. ([PyPI][1])
  * **Silero VAD** (PyTorch CPU) if you want more robust endpointing; still light and works at 16 kHz / 30 ms frames. ([Hugging Face][2])
* When VAD says “speech ended” (e.g., ≥500–800 ms of silence), send the buffered chunk to whisper via `pywhispercpp.Model(...).transcribe(..., single_segment=True, no_timestamps=True)` to get the command. ([PyPI][1])
* If you want **tight command vocab** (e.g., a dozen actions), you can guide decoding with whisper.cpp’s **GBNF grammar** (see C++ `examples/command`) or do a simple post-match over a whitelist. (The bindings let you pass most whisper.cpp params; for strict grammars the C++ example is the canonical reference.) ([GitHub][3])

### Metal / CoreML on Apple Silicon

* **whisper.cpp** itself supports **Metal**; when building native C/C++ examples you enable it with `-DGGML_USE_METAL`. ([GitHub][4])
* For the **Python bindings**, the documented accelerated path is **CoreML**: install from source with `WHISPER_COREML=1` (still on-device, very fast on M-series). If you absolutely need Metal specifically, call the compiled whisper.cpp **CLI** (built with Metal) from Python via `subprocess`, or stick with the CoreML backend through `pywhispercpp`. ([PyPI][1])

---

## Minimal working loop (Python)

This version uses `sounddevice` + `webrtcvad` for end-of-speech, and `pywhispercpp` for transcription. It’s tiny, battery-friendly, and “instant-feeling.”

```python
# pip install pywhispercpp sounddevice webrtcvad numpy
# (On macOS: brew install portaudio  # for sounddevice)

import queue, sys, time
import numpy as np
import sounddevice as sd
import webrtcvad

from pywhispercpp.model import Model

SAMPLE_RATE = 16000         # Whisper + VAD friendly
FRAME_MS = 30               # 30 ms frames for VAD
FRAME_SAMPLES = SAMPLE_RATE * FRAME_MS // 1000
SILENCE_AFTER_MS = 600      # end-of-speech threshold
MAX_UTTERANCE_SEC = 8       # optional safety cap

# 1) audio capture -> 16k mono float32 -> int16 for VAD
audio_q = queue.Queue()
def on_audio(indata, frames, time_info, status):
    if status: print(status, file=sys.stderr)
    mono = indata.mean(axis=1)                      # to mono
    resampled = mono                                # device default? set sd.Stream to 16k to avoid resample
    audio_q.put((resampled.copy(), time.time()))

# pick default mic; set dtype="float32", samplerate=16000 in stream
stream = sd.InputStream(callback=on_audio, channels=1, samplerate=SAMPLE_RATE, dtype="float32", blocksize=FRAME_SAMPLES)
vad = webrtcvad.Vad(2)  # 0-3: aggressiveness

# 2) init whisper (tiny.en for snappy; set single_segment & no_timestamps for “command” style)
w = Model('tiny.en', print_realtime=False, print_progress=False)  # or 'base.en' for a bit more accuracy

def frames_from_buffer(buf):
    # yields 30ms int16 frames for VAD
    pcm16 = np.clip(buf * 32768, -32768, 32767).astype(np.int16).tobytes()
    for i in range(0, len(pcm16), FRAME_SAMPLES*2):
        yield pcm16[i:i+FRAME_SAMPLES*2]

def recognize_command(audio_f32):
    # Whisper wants float32 PCM [-1,1]; we already have that
    segs = w.transcribe(
        audio_f32.astype(np.float32),
        single_segment=True,
        no_timestamps=True,
        language='en',
        # keep decoding tight to reduce latency:
        max_tokens=32,
        temperature=0.2,
        logprob_thold=-1.0,
        no_speech_thold=0.5,
    )
    text = "".join(s.text for s in segs).strip()
    return text

def intent_from_text(text):
    # toy matcher – replace with your intent router or a grammar
    t = text.lower()
    if "rotate" in t and ("right" in t or "clockwise" in t):
        return ("rotate_image", {"direction": "right"})
    if "rotate" in t and ("left" in t or "counter" in t):
        return ("rotate_image", {"direction": "left"})
    if "flip" in t and "horizontal" in t:
        return ("flip_image", {"axis": "horizontal"})
    return ("freeform_text", {"text": text})

def loop():
    with stream:
        print("Listening… (say a command, e.g., 'rotate the image to the right')")
        speech_buf = np.zeros(0, dtype=np.float32)
        last_voice = None
        utter_start = None

        while True:
            chunk, ts = audio_q.get()
            speech_buf = np.concatenate([speech_buf, chunk])

            # run VAD on 30ms frames over the newest audio only
            voiced_now = False
            for frm in frames_from_buffer(chunk):
                if len(frm) < FRAME_SAMPLES*2:
                    continue
                if vad.is_speech(frm, SAMPLE_RATE):
                    voiced_now = True

            now = time.time()
            if voiced_now:
                last_voice = now
                if utter_start is None:
                    utter_start = now

            # end-of-speech?
            if utter_start and last_voice and (now - last_voice) * 1000 >= SILENCE_AFTER_MS:
                # clamp utterance length
                n_samples = min(len(speech_buf), MAX_UTTERANCE_SEC * SAMPLE_RATE)
                audio = speech_buf[:n_samples]
                speech_buf = np.zeros(0, dtype=np.float32)
                last_voice = None
                utter_start = None

                text = recognize_command(audio)
                if text:
                    intent, payload = intent_from_text(text)
                    print(f"[ASR] {text}")
                    print(f"[INTENT] {intent} {payload}")

                    # >>> Call your app logic here, e.g.:
                    # if intent == "rotate_image": rotate_image(payload["direction"])

            # keep buffer from growing unbounded if user is silent
            if len(speech_buf) > SAMPLE_RATE * 30:
                speech_buf = speech_buf[-SAMPLE_RATE*10:]

if __name__ == "__main__":
    loop()
```

### Why this feels “instant”

* **Streaming capture** in 30 ms frames means VAD decides very quickly when you’ve stopped speaking. (WebRTC VAD is trained exactly for 10/20/30 ms frames.) ([PyPI][1])
* **Tiny Whisper model** + `single_segment=True` + low `max_tokens` keeps decoding tight. The assistant demo in pywhispercpp shows the same pattern. ([PyPI][1])
* Entirely **on-device** (Core Audio + CoreML/CPU); no network round-trip.

### Optional upgrades

* **Grammar / command-only decoding:** If you have a small set of commands, push the decoder with a **GBNF grammar** (see `examples/command`); this sharply reduces latency and wrong words. In Python, you can either (a) call the whisper.cpp `command` binary compiled with Metal from `subprocess`, or (b) post-filter as in `intent_from_text`. ([GitHub][3])
* **Silero VAD**: swap `webrtcvad` with Silero to improve endpointing in noisy rooms; same 16 kHz, 30 ms cadence. ([Hugging Face][2])
* **“Hotword + command” pattern**: require a wake phrase (e.g., “computer”) before accepting a command to avoid accidental triggers; the C++ command example supports prompts and thresholds you can mirror in Python. ([GitHub][3])

### Setup notes (macOS)

* `brew install portaudio` (for `sounddevice`), then `pip install sounddevice webrtcvad pywhispercpp`.
* If you want **accelerated** bindings: `WHISPER_COREML=1 pip install git+https://github.com/absadiki/pywhispercpp` (CoreML backend). ([PyPI][1])
* If you prefer **Metal** explicitly: build the **whisper.cpp** CLI with Metal (`-DGGML_USE_METAL`) and call it from Python, or stick to CoreML via pywhispercpp; both run fully on Apple Silicon. ([GitHub][4])

If you want, tell me your **full command list** (e.g., rotate, crop, zoom, adjust exposure), and I’ll add a tiny **GBNF grammar** or a smarter intent router so only those phrases are decoded—this makes it even snappier and more reliable.

[1]: https://pypi.org/project/pywhispercpp/ "pywhispercpp · PyPI"
[2]: https://huggingface.co/zohann/whisper-realtime-models/blob/main/silero-vad/README.md?utm_source=chatgpt.com "silero-vad/README.md · zohann/whisper-realtime-models at main"
[3]: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/command.cpp?utm_source=chatgpt.com "whisper.cpp/examples/command/command.cpp at master - GitHub"
[4]: https://github.com/ggerganov/whisper.cpp/blob/master/examples/whisper.objc/README.md?utm_source=chatgpt.com "whisper.cpp/examples/whisper.objc/README.md at master - GitHub"
